{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_sentiment_twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XoSHQILshO1P",
        "hdCaXhX2s3hu",
        "QcffdNkBvs35",
        "41CBt2dQwlT_",
        "xj0lj0aEuTRK",
        "69fHrD6lmp2q",
        "tpHGysZHTrrO",
        "J2I9gxaQactK"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZMv89ObEhSz",
        "outputId": "2de4ca0f-876c-4780-fffa-501fe44f68b2"
      },
      "source": [
        "# Optional link google drive, copy the code and enter.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoSHQILshO1P"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AVmGCoDvenk",
        "outputId": "9290a6e8-c5c7-4a54-d271-f3af3b42a369"
      },
      "source": [
        "!pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twint\n",
            "  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to /tmp/pip-install-9c17l7k9/twint_90871c991f7b4a54bde7a2b1ad3ea5f8\n",
            "  Running command git clone -q https://github.com/twintproject/twint.git /tmp/pip-install-9c17l7k9/twint_90871c991f7b4a54bde7a2b1ad3ea5f8\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q origin/master\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting aiodns\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from twint) (4.6.3)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-7.14.1-py2.py3-none-any.whl (363 kB)\n",
            "\u001b[K     |████████████████████████████████| 363 kB 26.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.7/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from twint) (1.1.5)\n",
            "Collecting aiohttp_socks\n",
            "  Downloading aiohttp_socks-0.6.0-py3-none-any.whl (9.2 kB)\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (from twint) (1.17.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Collecting googletransx\n",
            "  Downloading googletransx-2.4.2.tar.gz (13 kB)\n",
            "Collecting pycares>=4.0.0\n",
            "  Downloading pycares-4.0.0-cp37-cp37m-manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->twint) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.20)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 53.0 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.7.4.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (21.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.10)\n",
            "Collecting python-socks[asyncio]>=1.2.2\n",
            "  Downloading python_socks-1.2.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy->twint) (1.52)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from googletransx->twint) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->twint) (1.15.0)\n",
            "Building wheels for collected packages: twint, fake-useragent, googletransx\n",
            "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=38870 sha256=10b892ea271ae7c2e65f91000d1fbd56051e7893f6ea79a87ea042e5e2fdd2f7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h5swqvan/wheels/8d/dc/9f/74b4483d5f997036f04aec7f42bd4b3c80f04264920c368068\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=7c21c5548c94bbb3fbaa9a90d77cf432eadb35d88215fdb1963903e28feca965\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15968 sha256=aaddd630bc175bd3e530d6b7077efd3905f465092b004338dd472a6e6a9eb0d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/d5/b1/31104b338f7fd45aa8f7d22587765db06773b13df48a89735f\n",
            "Successfully built twint fake-useragent googletransx\n",
            "Installing collected packages: multidict, yarl, python-socks, async-timeout, pycares, aiohttp, schedule, googletransx, fake-useragent, elasticsearch, dataclasses, cchardet, aiohttp-socks, aiodns, twint\n",
            "\u001b[33m  WARNING: The script twint is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed aiodns-3.0.0 aiohttp-3.7.4.post0 aiohttp-socks-0.6.0 async-timeout-3.0.1 cchardet-2.1.7 dataclasses-0.6 elasticsearch-7.14.1 fake-useragent-0.1.11 googletransx-2.4.2 multidict-5.1.0 pycares-4.0.0 python-socks-1.2.4 schedule-1.1.0 twint-2.1.21 yarl-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74xE_9mUBQvF",
        "outputId": "5e84e01d-c75c-42c0-c24c-91985cc6ee04"
      },
      "source": [
        "!pip install alpaca-trade-api"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alpaca-trade-api\n",
            "  Downloading alpaca_trade_api-1.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▋                        | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 30 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 40 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 906 kB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack==1.0.2 in /usr/local/lib/python3.7/dist-packages (from alpaca-trade-api) (1.0.2)\n",
            "Collecting PyYAML==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from alpaca-trade-api) (1.1.5)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.7/dist-packages (from alpaca-trade-api) (1.24.3)\n",
            "Collecting aiohttp==3.7.4\n",
            "  Downloading aiohttp-3.7.4-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>2 in /usr/local/lib/python3.7/dist-packages (from alpaca-trade-api) (2.23.0)\n",
            "Collecting websocket-client<2,>=0.56.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting websockets<10,>=8.0\n",
            "  Downloading websockets-9.1-cp37-cp37m-manylinux2010_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from alpaca-trade-api) (1.19.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /root/.local/lib/python3.7/site-packages (from aiohttp==3.7.4->alpaca-trade-api) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /root/.local/lib/python3.7/site-packages (from aiohttp==3.7.4->alpaca-trade-api) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4->alpaca-trade-api) (3.7.4.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4->alpaca-trade-api) (21.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /root/.local/lib/python3.7/site-packages (from aiohttp==3.7.4->alpaca-trade-api) (5.1.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.4->alpaca-trade-api) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.1->alpaca-trade-api) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.18.1->alpaca-trade-api) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>2->alpaca-trade-api) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>2->alpaca-trade-api) (2021.5.30)\n",
            "Installing collected packages: websockets, websocket-client, PyYAML, aiohttp, alpaca-trade-api\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgMMBlC6Bo_L",
        "outputId": "f1f7d222-48fc-413e-a0f3-941ac6797fb4"
      },
      "source": [
        "!pip3 install python-dotenv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.19.0-py2.py3-none-any.whl (17 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4Ml6yqRTJLY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "960a03e0-e3a3-4a27-af60-ccf5528c30ed"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import string\n",
        "import datetime\n",
        "import numpy as np\n",
        "import twint\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "\n",
        "from bokeh.models import ColumnDataSource, HoverTool\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from keras.backend import manual_variable_initialization\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import alpaca_trade_api as tradeapi\n",
        "\n",
        "import requests\n",
        "\n",
        "nest_asyncio.apply()\n",
        "manual_variable_initialization(True)\n",
        "\n",
        "# Call once to configure Bokeh to display plots inline in the notebook.\n",
        "output_notebook()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9e46dbf5b3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtwint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twint'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaiLRl0isNPg"
      },
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  \n",
        "  labels = dataframe.pop('target')\n",
        "  \n",
        "  features_dataset = tf.data.Dataset.from_tensor_slices(dataframe)\n",
        "  labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "  ds = tf.data.Dataset.zip((features_dataset, labels_dataset))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds\n",
        "\n",
        "# A utility method to vectorize our tweets labeled dataset\n",
        "def int_vectorize_text(text, label):\n",
        "  return encoder(text), label\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Configure for model\n",
        "def configure_dataset(dataset):\n",
        "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Print prediction examples\n",
        "def print_my_examples(inputs, results):\n",
        "  if isinstance(inputs, pd.Series):\n",
        "    inputs.reset_index(inplace=True)\n",
        "  result_for_printing = \\\n",
        "    [f'input: {inputs[i]:<30}\\nscore: {results[i][0]:.6f}\\n'\n",
        "                         for i in range(len(inputs))]\n",
        "  print(*result_for_printing, sep='\\n')\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj8sdaObIrN8"
      },
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 10000\n",
        "sequence_length = 140\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Note that the layer uses the custom standardization defined above.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "encoder = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmepVF14i4ZP"
      },
      "source": [
        "# Read the twitter sentiment data\n",
        "twitter_df = pd.read_csv(\n",
        "    './gdrive/MyDrive/twitter-sentiment/twitter.csv',\n",
        "    encoding =\"ISO-8859-1\",\n",
        "    names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "twitter_df = twitter_df.drop(columns=['ids', 'date', 'flag', 'user'])\n",
        "\n",
        "# In the original dataset \"4\" indicates the pet was not adopted.\n",
        "twitter_df['target'] = np.where(twitter_df['target']==4, 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-5a9QQqcLvm"
      },
      "source": [
        "# View head and target counts\n",
        "display(twitter_df.head())\n",
        "twitter_df.target.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqV38ktFcj6r"
      },
      "source": [
        "# Create train/val/test splits\n",
        "train, test = train_test_split(twitter_df, test_size=0.2, random_state=3)\n",
        "train, val = train_test_split(twitter_df, test_size=0.2, random_state=3)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn1qkJB5JjLh"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "raw_train_ds = df_to_dataset(train, shuffle=False, batch_size=BATCH_SIZE)\n",
        "raw_val_ds = df_to_dataset(val, shuffle=False, batch_size=BATCH_SIZE)\n",
        "raw_test_ds = df_to_dataset(test, shuffle=False, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaqwXzz7Ji-o"
      },
      "source": [
        "for text, target in raw_train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print('text: ', text[i].numpy())\n",
        "    print('target: ', target[i].numpy())\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbr4Mt18Mjhq"
      },
      "source": [
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = raw_train_ds.map(lambda text, target: text)\n",
        "encoder.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOt6OOq6nvh3"
      },
      "source": [
        "# Review the vocab\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]\n",
        "\n",
        "# Review twitter encoded/decoded data\n",
        "for text, label in raw_train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    encoded_example = encoder(text.numpy()[i])\n",
        "    print(\"Original:\\t\", text.numpy()[i])\n",
        "    print(\"Round-trip:\\t\", \" \".join(vocab[encoded_example]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2IgVF4njuJC"
      },
      "source": [
        "train_ds = configure_dataset(raw_train_ds.map(int_vectorize_text))\n",
        "val_ds = configure_dataset(raw_val_ds.map(int_vectorize_text))\n",
        "test_ds = configure_dataset(raw_test_ds.map(int_vectorize_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdCaXhX2s3hu"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnJ2s_qVGZtt"
      },
      "source": [
        "# Base model setup\n",
        "embedding_dim = 16\n",
        "date_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "log_dir = f'./gdrive/MyDrive/twitter-sentiment/base-model/logs/{date_time}'\n",
        "model_save_dir = f'./gdrive/MyDrive/twitter-sentiment/base-model/model/{date_time}'\n",
        "embedding_dir = '/content/gdrive/MyDrive/twitter-sentiment/base-model/embeddings/' + date_time\n",
        "\n",
        "os.mkdir(embedding_dir)\n",
        "\n",
        "model_basic = Sequential([\n",
        "  Embedding(vocab_size, embedding_dim, name='embedding'),\n",
        "  GlobalAveragePooling1D(),\n",
        "  Dense(16, activation='relu'),\n",
        "  Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjFON_oUJUGS"
      },
      "source": [
        "# Setup tensorboard logs\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LaJRm1yNc87"
      },
      "source": [
        "# Compile base model\n",
        "model_basic.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# tf.keras.metrics.FalseNegatives()\n",
        "# tf.keras.metrics.FalsePositives()\n",
        "# tf.keras.metrics.TrueNegatives()\n",
        "# tf.keras.metrics.TruePositives()\n",
        "# tf.keras.metrics.Precision()\n",
        "# tf.keras.metrics.Recall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdnPDok6g0ob"
      },
      "source": [
        "# View summary\n",
        "model_basic.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PhVjLujNf7V"
      },
      "source": [
        "# Fit base model\n",
        "model_basic.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        "    callbacks=[tensorboard_callback])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmMOhu7pOKKA"
      },
      "source": [
        "print(\"Evaluate\")\n",
        "result = model_basic.evaluate(test_ds)\n",
        "dict(zip(model_basic.metrics_names, result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd7GSDZQu_M0"
      },
      "source": [
        "# Save base model\n",
        "model_basic.save(model_save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8NDfd6AN9n2"
      },
      "source": [
        "# View tensorboard saved info\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./gdrive/MyDrive/twitter-sentiment/base-model/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkzYOBLkOdoE"
      },
      "source": [
        "# Save weights to view on tensorflow projector\n",
        "weights = model_basic.get_layer('embedding').get_weights()[0]\n",
        "vocab = encoder.get_vocabulary()\n",
        "\n",
        "# http://projector.tensorflow.org/\n",
        "out_v = io.open(f'{embedding_dir}/vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open(f'{embedding_dir}/metadate.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcffdNkBvs35"
      },
      "source": [
        "## RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQERe_TkeWuW"
      },
      "source": [
        "# RNN model setup\n",
        "embedding_dim = 16\n",
        "date_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "log_dir = f'./gdrive/MyDrive/twitter-sentiment/rnn-model/logs/{date_time}'\n",
        "model_save_dir = f'./gdrive/MyDrive/twitter-sentiment/rnn-model/model/{date_time}'\n",
        "embedding_dir = '/content/gdrive/MyDrive/twitter-sentiment/rnn-model/embeddings/' + date_time\n",
        "\n",
        "os.mkdir(embedding_dir)\n",
        "\n",
        "# RNN model setup\n",
        "model_rnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=64,\n",
        "        name='embedding',\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhqh2CvMwAZp"
      },
      "source": [
        "# Setup tensorboard logs\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJhrBXaxhAm4"
      },
      "source": [
        "# Compile RNN model\n",
        "model_rnn.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3gk0-sIhuxx"
      },
      "source": [
        "# View summary\n",
        "model_rnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIChjQwihDFK"
      },
      "source": [
        "# Fit RNN model\n",
        "history = model_rnn.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT7lLNxMc8UR"
      },
      "source": [
        "print(\"Evaluate\")\n",
        "result = model_rnn.evaluate(test_ds)\n",
        "dict(zip(model_rnn.metrics_names, result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5dKiDa_wRDg"
      },
      "source": [
        "# Save RNN model\n",
        "model_rnn.save(model_save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK09fR2fkD9x"
      },
      "source": [
        "# View tensorboard saved info\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir ./gdrive/MyDrive/twitter-sentiment/rnn-model/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7D6LVAkwp8Y"
      },
      "source": [
        "# Save weights to view on tensorflow projector\n",
        "weights = model_rnn.get_layer('embedding').get_weights()[0]\n",
        "vocab = encoder.get_vocabulary()\n",
        "\n",
        "# http://projector.tensorflow.org/\n",
        "out_v = io.open(f'{embedding_dir}/vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open(f'{embedding_dir}/metadate.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41CBt2dQwlT_"
      },
      "source": [
        "## RNN 2-layer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8oW1Vh1mC5O"
      },
      "source": [
        "# RNN model setup\n",
        "embedding_dim = 16\n",
        "date_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "log_dir = f'./gdrive/MyDrive/twitter-sentiment/rnn2-model/logs/{date_time}'\n",
        "model_save_dir = f'./gdrive/MyDrive/twitter-sentiment/rnn2-model/model/{date_time}'\n",
        "embedding_dir = '/content/gdrive/MyDrive/twitter-sentiment/rnn2-model/embeddings/' + date_time\n",
        "\n",
        "os.mkdir(embedding_dir)\n",
        "\n",
        "# RNN 2-layer model setup\n",
        "model_rnn_2_layer = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True, name='embedding'),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qApWl7xkxoaa"
      },
      "source": [
        "# Setup tensorboard logs\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeIZTBO7xwi3"
      },
      "source": [
        "# Compile RNN 2-layer model\n",
        "model_rnn_2_layer.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq9P_Ei9x163"
      },
      "source": [
        "# View summary\n",
        "model_rnn_2_layer.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pLZNnL_yOT_"
      },
      "source": [
        "# Fit RNN 2-layer model\n",
        "history = model_rnn_2_layer.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5,\n",
        "    callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJCbQaKx5XUr"
      },
      "source": [
        "print(\"Evaluate\")\n",
        "result = model_rnn2.evaluate(test_ds)\n",
        "dict(zip(model_rnn2.metrics_names, result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG0VO-IAyZS9"
      },
      "source": [
        "# Save RNN 2-layer model\n",
        "model_rnn_2_layer.save(model_save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHzqdMecyeib"
      },
      "source": [
        "# View tensorboard saved info\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir ./gdrive/MyDrive/twitter-sentiment/rnn2-model/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPQSLRuPyjp5"
      },
      "source": [
        "# Save weights to view on tensorflow projector\n",
        "weights = model_rnn_2_layer.get_layer('embedding').get_weights()[0]\n",
        "vocab = encoder.get_vocabulary()\n",
        "\n",
        "# http://projector.tensorflow.org/\n",
        "out_v = io.open(f'{embedding_dir}/vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open(f'{embedding_dir}/metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noyg6TmIFeFp"
      },
      "source": [
        "# View tensorboard saved info\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir ./gdrive/MyDrive/twitter-sentiment/rnn2-model/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj0lj0aEuTRK"
      },
      "source": [
        "## Load Models Back for Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyJv7WyFlpeT"
      },
      "source": [
        "# text:  [b'@umeshunni yeah iPhone does have more going for it.. now thats decided.. all thats left is wait for 3G here.. ']\n",
        "# target:  1\n",
        "\n",
        "# text:  [b'@BenjaminHouston oh I wish you guys were coming here  I wanna see savvy!!! ( and you guys of course) ha']\n",
        "# target:  1\n",
        "\n",
        "# text:  [b'@CaliSmiles06 hmm. ;) yuh, up &amp; out with the #rat boys &amp; girls. ']\n",
        "# target:  0\n",
        "\n",
        "# text:  [b'never thought my daughter would want to borrow my clothes... even less that she would suggest that i borrow hers... luckily it didnt fit ']\n",
        "# target:  0\n",
        "\n",
        "# text:  [b\"@xxashwee25  i can't stop touching it cause its so thin! when i put it in a pony tail is like soo tiny compared to usual.\"]\n",
        "# target:  0\n",
        "examples = ['@umeshunni yeah iPhone does have more going for it.. now thats decided.. all thats left is wait for 3G here.. ',\n",
        "            '@BenjaminHouston oh I wish you guys were coming here  I wanna see savvy!!! ( and you guys of course) ha',\n",
        "            '@CaliSmiles06 hmm. ;) yuh, up &amp; out with the #rat boys &amp; girls. ',\n",
        "            'never thought my daughter would want to borrow my clothes... even less that she would suggest that i borrow hers... luckily it didnt fit ',\n",
        "            \"@xxashwee25  i can't stop touching it cause its so thin! when i put it in a pony tail is like soo tiny compared to usual.\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0T6HyBSuVjk"
      },
      "source": [
        "model_basic_reloaded = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/base-model/model/' + os.listdir('./gdrive/MyDrive/twitter-sentiment/base-model/model')[-1])\n",
        "model_rnn_reloaded = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/rnn-model/model/' + os.listdir('./gdrive/MyDrive/twitter-sentiment/rnn-model/model')[-1])\n",
        "mode_rnn_2_layer_reloaded = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/rnn2-model/model/' + os.listdir('./gdrive/MyDrive/twitter-sentiment/rnn2-model/model')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZGZeoVTHF1Z"
      },
      "source": [
        "predictions = tf.sigmoid(model_basic_reloaded(tf.constant(encoder(examples))))\n",
        "print_my_examples(examples, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69fHrD6lmp2q"
      },
      "source": [
        "## Load Models for Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX5MezhBmpEJ"
      },
      "source": [
        "# most_recent_base_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/base-model/model/' + os.listdir('./gdrive/MyDrive/twitter-sentiment/base-model/model')[-1])\n",
        "best_base_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/base-model/model/20210913-035233')\n",
        "# test_base_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/base-model/model/20210910-181817')\n",
        "\n",
        "# most_recent_rnn_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/rnn-model/model/' + os.listdir('./gdrive/MyDrive/twitter-sentiment/rnn-model/model')[-1])\n",
        "best_rnn_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/rnn-model/model/20210913-043938')\n",
        "# test_rnn_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/rnn-model/model/20210910-181817')\n",
        "\n",
        "# most_recent_rnn2_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/rnn2-model/model/' + os.listdir('./gdrive/MyDrive/twitter-sentiment/rnn2-model/model')[-1])\n",
        "best_rnn2_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/rnn2-model/model/20210913-064343')\n",
        "# test_rnn2_model = keras.models.load_model(f'/content/gdrive/MyDrive/twitter-sentiment/rnn2-model/model/20210910-181817')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKHskLwmSnkv"
      },
      "source": [
        "current_working_model = best_rnn2_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFNdzvufyzJ0"
      },
      "source": [
        "## Add ATVI Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJHW-XQuHKX_"
      },
      "source": [
        "  # # Configure\n",
        "  # for i in range(len(df)-1):\n",
        "  #   date_time = df.loc[i,'Date']\n",
        "  #   c = twint.Config()\n",
        "  #   c.Search = \"#blizzard\"\n",
        "  #   c.Since = df.loc[i,'Date'].strftime(\"%Y-%m-%d\")\n",
        "  #   c.Until = df.loc[i+1,'Date'].strftime(\"%Y-%m-%d\")\n",
        "  #   c.Store_csv = True\n",
        "  #   c.Lang = \"en\"\n",
        "  #   c.Output = f'./gdrive/MyDrive/twitter-sentiment/{c.Search}-{date_time}.csv'\n",
        "\n",
        "  #   # Run\n",
        "  #   try:\n",
        "  #     twint.run.Search(c)\n",
        "  #   catch:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZCII7_V6zcn"
      },
      "source": [
        "pulled_tweets = pd.read_csv('/content/gdrive/MyDrive/twitter-sentiment/combined_csv.csv', index_col='created_at', parse_dates=True, infer_datetime_format=True)\n",
        "pulled_tweets.reset_index(inplace=True)\n",
        "\n",
        "pulled_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DyAQKaI6ebb"
      },
      "source": [
        "# Pull only english tweets\n",
        "pulled_tweets = pulled_tweets[pulled_tweets['language']=='en']\n",
        "pulled_tweets.reset_index()\n",
        "pulled_tweets = pulled_tweets.set_index('created_at')\n",
        "pulled_tweets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apjONnpzzFC8"
      },
      "source": [
        "# Add predictions to pulled_tweets df\n",
        "predictions = tf.sigmoid(current_working_model.predict(tf.constant(encoder(pulled_tweets['tweet']))))\n",
        "predictions_list = []\n",
        "for prediction in predictions:\n",
        "  predictions_list.append(tf.keras.backend.get_value(prediction[0]))\n",
        "pulled_tweets['prediction'] = predictions_list\n",
        "pulled_tweets.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJoN2V85xeBF"
      },
      "source": [
        "# Create new dataframe of signals we want to include\n",
        "\n",
        "\n",
        "signals = pulled_tweets[['tweet', 'prediction']].copy()\n",
        "\n",
        "\n",
        "# signals = pulled_tweets[['tweet', 'prediction']].copy()\n",
        "\n",
        "\n",
        "display(signals.head())\n",
        "\n",
        "def center_prediction(prediction):\n",
        "  if prediction == 0.5:\n",
        "    return 0\n",
        "  elif prediction > 0.5:\n",
        "    return (prediction - 0.5)\n",
        "  else:\n",
        "    return -(.5-prediction)\n",
        "\n",
        "# Cleanup prediction column\n",
        "signals['prediction'] = signals['prediction'].apply(lambda x : center_prediction(x))\n",
        "#signals.set_index('created_at')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvTU1W_nyWb6"
      },
      "source": [
        "# Groupby date and just do a simple sum.....\n",
        "# Could do it this way, i.e. with just a simple sum or by creating a threshold of the prediciton and then label each as 1 or 0\n",
        "short_window = 12\n",
        "long_window = 26\n",
        "signal_window = 9\n",
        "\n",
        "#signals = pd.DataFrame(signals.resample('D', on='created_at').prediction.sum())\n",
        "signals = pd.DataFrame(signals.resample('D').prediction.sum())\n",
        "# signals[f'prediction-{short_window}'] = signals['prediction'].rolling(short_window).mean()\n",
        "# signals[f'prediction-{long_window}'] = signals['prediction'].rolling(long_window).mean()\n",
        "\n",
        "# MACD calc\n",
        "exp1 = signals['prediction'].ewm(span=12, adjust=False).mean()\n",
        "exp2 = signals['prediction'].ewm(span=26, adjust=False).mean()\n",
        "signals['MACD'] = macd = exp1 - exp2\n",
        "signals['signal'] = exp3 = macd.ewm(span=signal_window, adjust=False).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjLtZHaNLjVa"
      },
      "source": [
        "signals = signals.dropna()\n",
        "signals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq95LZqDzWZe"
      },
      "source": [
        "# Changed the labels on the graph to more accurately What was being graphed\n",
        "\n",
        "p = figure(\n",
        "    title=\"Prediction Daily\",\n",
        "    x_axis_label='Date',\n",
        "    x_axis_type='datetime',\n",
        "    y_axis_label='Prediction sum',\n",
        "    plot_width=1000,\n",
        "    plot_height=400)\n",
        "p.line(signals.index, signals['MACD'], legend_label=f\"{short_window} - {long_window} Day Rolling MACD\", line_color='green')\n",
        "p.line(signals.index, signals['signal'], legend_label=f\"{signal_window} Day Rolling Signal Line\")\n",
        "show(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHGysZHTrrO"
      },
      "source": [
        "## Add NASDAQ 100 Twitter Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu8JHwe6Twks"
      },
      "source": [
        "# nasdaq_tweets = pd.read_csv('./gdrive/MyDrive/twitter-sentiment/nasdaq100-tickers-combined.csv', \n",
        "#                             parse_dates=True, \n",
        "#                             infer_datetime_format=True\n",
        "#                             )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c6mEjgEqQyQ"
      },
      "source": [
        "# display(nasdaq_tweets.head())\n",
        "# display(nasdaq_tweets.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRwoDJfdqpdN"
      },
      "source": [
        "# # Pull only english tweets\n",
        "# nasdaq_tweets_en = nasdaq_tweets[nasdaq_tweets['Tweet language (ISO 639-1)']=='en']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFxDkGiFszee"
      },
      "source": [
        "# # Add predictions to pulled_tweets df\n",
        "# predictions_nasdaq = tf.sigmoid(current_working_model.predict(tf.constant(encoder(nasdaq_tweets_en['Tweet content']))))\n",
        "# nasdaq_predictions_list = []\n",
        "# for prediction in predictions_nasdaq:\n",
        "#   nasdaq_predictions_list.append(tf.keras.backend.get_value(prediction[0]))\n",
        "# nasdaq_tweets_en['prediction'] = nasdaq_predictions_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Reh_oTC8VB2"
      },
      "source": [
        "# nasdaq_tweets_en.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DM4ftxLAkNC"
      },
      "source": [
        "# nasdaq_tweets_en.to_csv(f'/content/gdrive/MyDrive/twitter-sentiment/nasdaq-tweets-cleaned-rnn2-20210913-064343', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzH135h7BTdd"
      },
      "source": [
        "cleaned_nasdaq_predictions = pd.read_csv(\n",
        "    '/content/gdrive/MyDrive/twitter-sentiment/nasdaq-tweets-cleaned-rnn2-20210913-064343',\n",
        "    parse_dates=True,\n",
        "    infer_datetime_format=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hjMS1vABg-9"
      },
      "source": [
        "cleaned_nasdaq_predictions['Date'] = pd.to_datetime(cleaned_nasdaq_predictions['Date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-SkvXHACN7P"
      },
      "source": [
        "# Create new dataframe of signals we want to include\n",
        "nasdaq_signals = cleaned_nasdaq_predictions[['Date', 'Tweet content', 'prediction', 'ticker']].copy()\n",
        "\n",
        "display(nasdaq_signals.head())\n",
        "\n",
        "def center_prediction(prediction):\n",
        "  if prediction == 0.5:\n",
        "    return 0\n",
        "  elif prediction > 0.5:\n",
        "    return (prediction - 0.5)\n",
        "  else:\n",
        "    return -(.5-prediction)\n",
        "\n",
        "# Cleanup prediction column\n",
        "nasdaq_signals['prediction'] = nasdaq_signals['prediction'].apply(lambda x : center_prediction(x))\n",
        "nasdaq_signals.set_index('Date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23I0TVUXTb1J"
      },
      "source": [
        "# Groupby date and just do a simple sum.....\n",
        "# Could do it this way, i.e. with just a simple sum or by creating a threshold of the prediciton and then label each as 1 or 0\n",
        "short_window = 12\n",
        "long_window = 26\n",
        "signal_window = 9\n",
        "\n",
        "nasdaq_signals = pd.DataFrame(nasdaq_signals.resample('D', on='Date').prediction.sum())\n",
        "# signals[f'prediction-{short_window}'] = signals['prediction'].rolling(short_window).mean()\n",
        "# signals[f'prediction-{long_window}'] = signals['prediction'].rolling(long_window).mean()\n",
        "\n",
        "# MACD calc\n",
        "exp1 = nasdaq_signals['prediction'].ewm(span=12, adjust=False).mean()\n",
        "exp2 = nasdaq_signals['prediction'].ewm(span=26, adjust=False).mean()\n",
        "nasdaq_signals['MACD'] = macd = exp1 - exp2\n",
        "nasdaq_signals['signal'] = exp3 = macd.ewm(span=signal_window, adjust=False).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgrMx_KIaR7p"
      },
      "source": [
        "nasdaq_signals = nasdaq_signals.dropna()\n",
        "nasdaq_signals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2I9gxaQactK"
      },
      "source": [
        "## FinViz BeautifulSoup Scraper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONLmlMXTakqZ"
      },
      "source": [
        "# # BeautifulSoup scraper example to adapt\n",
        "# # https://www.tandfonline.com/doi/full/10.1080/24751839.2021.1874252\n",
        "# website_url = 'https://finviz.com/quote.ashx?t='\n",
        "\n",
        "# company_tikcers = ['ATVI']\n",
        "# news_tables = {}\n",
        "# parsed_data = []\n",
        "# params = {}\n",
        "\n",
        "# for ticker in company_tikcers:\n",
        "#   url = website_url + ticker\n",
        "#   res = requests.get(url, params=params)\n",
        "#   response = res.dump\n",
        "#   html = BeautifulSoup(response, 'html')\n",
        "#   news_data = html.find(id='news-table')\n",
        "#   news_tables[ticker] = news_data\n",
        "\n",
        "# for ticker, news_table in news_tables.items():\n",
        "#   for row in news_table.findAll('tr'):\n",
        "#     title = row.a.text\n",
        "#     date_data = row.td.text.split('')\n",
        "#     if len(date_data) == 1:\n",
        "#       time = date_data[0][0:7]\n",
        "#     else:\n",
        "#       date = datetime.datetime.strptime(date_data[0], '%b-%d-%y').strftime('%Y/%m/%d')\n",
        "#       time = date_data[1][0:7]\n",
        "#       parsed_data.append([ticker, date, time, title])\n",
        "\n",
        "# dataset = pd.DataFrame(parsed_data, columns=['Company', 'Date', 'Time', 'News Headline'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNoBdGZNJSFS"
      },
      "source": [
        "## Alpaca API- Stock Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Lttz8ANI2J"
      },
      "source": [
        "# Upload your env file containing your API secret key.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQrfrWq0JRXj"
      },
      "source": [
        "load_dotenv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ0lmuvWKRU1"
      },
      "source": [
        "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
        "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
        "\n",
        "# Check the values were imported correctly by evaluating the type of each\n",
        "display(type(alpaca_api_key))\n",
        "display(type(alpaca_secret_key))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQvOeu1CKRbn"
      },
      "source": [
        "# Create your Alpaca API REST object by calling Alpaca's tradeapi.REST function\n",
        "# Set the parameters to your alpaca_api_key, alpaca_secret_key and api_version=\"v2\" \n",
        "alpaca = tradeapi.REST(\n",
        "    alpaca_api_key,\n",
        "    alpaca_secret_key,\n",
        "    api_version=\"v2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeLs---_KReu"
      },
      "source": [
        "# Create the list for the required tickers\n",
        "tickers = [\"ATVI\", \"SPY\"]\n",
        "nasdaq100_set = {'aal',\n",
        " 'aapl',\n",
        " 'adbe',\n",
        " 'adp',\n",
        " 'akam',\n",
        " 'alxn',\n",
        " 'amat',\n",
        " 'amgn',\n",
        " 'amzn',\n",
        " 'atvi',\n",
        " 'avgo',\n",
        " 'bbby',\n",
        " 'bidu',\n",
        " 'bmrn',\n",
        " 'ca',\n",
        " 'celg',\n",
        " 'cern',\n",
        " 'chkp',\n",
        " 'chtr',\n",
        " 'cmcsa',\n",
        " 'cost',\n",
        " 'csco',\n",
        " 'csx',\n",
        " 'ctrp',\n",
        " 'ctsh',\n",
        " 'disca',\n",
        " 'disck',\n",
        " 'dish',\n",
        " 'dltr',\n",
        " 'ea',\n",
        " 'ebay',\n",
        " 'endp',\n",
        " 'esrx',\n",
        " 'expe',\n",
        " 'fast',\n",
        " 'fb',\n",
        " 'fisv',\n",
        " 'fox',\n",
        " 'foxa',\n",
        " 'gild',\n",
        " 'goog',\n",
        " 'googl',\n",
        " 'hsic',\n",
        " 'ilmn',\n",
        " 'inct',\n",
        " 'incy',\n",
        " 'intu',\n",
        " 'isrg',\n",
        " 'jd',\n",
        " 'khc',\n",
        " 'lbtya',\n",
        " 'lbtyk',\n",
        " 'lltc',\n",
        " 'lmca',\n",
        " 'lmck',\n",
        " 'lrcx',\n",
        " 'lvnta',\n",
        " 'mar',\n",
        " 'mat',\n",
        " 'mdlz',\n",
        " 'mnst',\n",
        " 'mu',\n",
        " 'mxim',\n",
        " 'myl',\n",
        " 'nclh',\n",
        " 'nflx',\n",
        " 'ntes',\n",
        " 'nvda',\n",
        " 'nxpi',\n",
        " 'orly',\n",
        " 'payx',\n",
        " 'pcar',\n",
        " 'pcln',\n",
        " 'pypl',\n",
        " 'qcom',\n",
        " 'qvca',\n",
        " 'regn',\n",
        " 'rost',\n",
        " 'sbac',\n",
        " 'sbux',\n",
        " 'sndk',\n",
        " 'srcl',\n",
        " 'stx',\n",
        " 'swks',\n",
        " 'symc',\n",
        " 'tmus',\n",
        " 'trip',\n",
        " 'tsco',\n",
        " 'tsla',\n",
        " 'txn',\n",
        " 'ulta',\n",
        " 'viab',\n",
        " 'vod',\n",
        " 'vrsk',\n",
        " 'vrtx',\n",
        " 'wdc',\n",
        " 'wfm',\n",
        " 'xlnx',\n",
        " 'yhoo'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujVChlJvNmqe"
      },
      "source": [
        "# Set the values for start_date and end_date using the pd.Timestamp function\n",
        "# The start and end data should be 2019-05-01 to 2020-05-01\n",
        "# Set the parameter tz to \"America/New_York\", \n",
        "# Set this all to the ISO format by calling the isoformat function \n",
        "start_date = pd.Timestamp(\"2020-09-10\", tz= \"America/New_York\").isoformat()\n",
        "end_date = pd.Timestamp(\"2021-09-10\", tz= \"America/New_York\").isoformat()\n",
        "timeframe = '1D'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAcY9UU9NmtR"
      },
      "source": [
        "# Use the Alpaca get_barset function to gather the price information for each ticker\n",
        "# Include the function parameters: tickers, timeframe, start, and end\n",
        "# Be sure to call the df property to ensure that the returned information is set as a DataFrame\n",
        "prices_df = alpaca.get_barset(\n",
        "    tickers,\n",
        "    timeframe,\n",
        "    start=start_date,\n",
        "    end=end_date,\n",
        "    limit=1000\n",
        ").df\n",
        "\n",
        "\n",
        "# Review the first five rows of the resulting DataFrame \n",
        "# YOUR CODE HERE \n",
        "display(prices_df.head())\n",
        "display(prices_df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJJ_UkG5BV1w"
      },
      "source": [
        "# prices_df.to_csv('/content/gdrive/MyDrive/twitter-sentiment/ATVI-SPY-year-prices', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aJkhrAoBtzh"
      },
      "source": [
        "# ATVI_SPY_prices = pd$.read_csv('/content/gdrive/MyDrive/twitter-sentiment/ATVI-SPY-year-prices')\n",
        "# ATVI_SPY_prices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA8XfaDYNmvG"
      },
      "source": [
        "# Create an empty DataFrame for holding the closing prices\n",
        "closing_prices_df = pd.DataFrame()\n",
        "\n",
        "# Using a for loop, for every ticker in the tickers list, \n",
        "# Select the close price for each ticker in the prices_df Dataframe\n",
        "# That will be set equal to closing_prices_df for the same ticker value\n",
        "for ticker in tickers:\n",
        "    closing_prices_df[ticker] = prices_df[ticker]['close']\n",
        "\n",
        "# For the new closing_prices_df DataFrame, keep only the date component\n",
        "closing_prices_df.index = closing_prices_df.index.date\n",
        "\n",
        "# View the first and last five rows of the closing_prices_df DataFrame\n",
        "display(closing_prices_df.head())\n",
        "display(closing_prices_df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPDOdqiO07f5"
      },
      "source": [
        "# TOOLTIPS = [\n",
        "#     (\"index\", \"$index\"),\n",
        "#     (\"(x,y)\", \"($x, $y)\"),\n",
        "#     (\"close\", \"$@{ATVI}\"),\n",
        "# ]\n",
        "\n",
        "# # source = ColumnDataSource(data={\n",
        "# #     'id'      : pulled_tweets['id'],\n",
        "# #     'tweet' : pulled_tweets['tweet']\n",
        "# # })\n",
        "\n",
        "# # Plot close price and tweets\n",
        "# p = figure(\n",
        "#     title=\"Price and Tweets\",\n",
        "#     x_axis_label='Date',\n",
        "#     x_axis_type='datetime',\n",
        "#     y_axis_label='Price',\n",
        "#     plot_width=1000,\n",
        "#     plot_height=400,\n",
        "#     tooltips = TOOLTIPS)\n",
        "# p.line(\n",
        "#     closing_prices_df.index,\n",
        "#     closing_prices_df['ATVI'],\n",
        "#     legend_label=f'ATVI - close price',\n",
        "#     line_color='green')\n",
        "# p.asterisk(pulled_tweets.index, closing_prices_df['ATVI'], size=20, color=\"#F0027F\")\n",
        "# # p.line(signals.index, signals['signal'], legend_label=f\"{signal_window} Day Rolling Signal Line\")\n",
        "# show(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oodNdTXNmxv"
      },
      "source": [
        "# Compute daily returns of the closing_prices_df DataFrame using the pct_change function \n",
        "# Be sure to drop the first row of NaN values\n",
        "daily_returns_df = closing_prices_df.pct_change().dropna()\n",
        "# Review the first and last five rows of the daily_returns_df DataFrame\n",
        "display(daily_returns_df.head())\n",
        "display(daily_returns_df.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxlBJ6orQYze"
      },
      "source": [
        "# Using the Pandas describe function, generate summary statistics\n",
        "# for each of the tickers in the daily_returns_df DataFrame\n",
        "# YOUR CODE HERE \n",
        "daily_returns_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpwluXEoQY15"
      },
      "source": [
        "daily_returns_df.plot.density(figsize=(15,7), title=\"Daily Returns for SPY and ATVI\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKRboMElM35x"
      },
      "source": [
        "# Basic Long Short Trade Bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFYa_z-1NB2e"
      },
      "source": [
        "# Concat together the basic version of a trading bot using twitter sentiment\n",
        "df_trades_one = pd.concat([signals, daily_returns_df[\"ATVI\"]], axis=\"columns\").copy().dropna()\n",
        "\n",
        "# Create our long and short signal, I used a reversion to mean strategy that seemed to work very well on this base level, will have to test on a few other tickers to be sure\n",
        "df_trades_one[\"trade_signal\"] = 0\n",
        "\n",
        "df_trades_one.loc[( (df_trades_one[\"MACD\"] - df_trades_one[\"signal\"]) <= 0 ), \"trade_signal\"] = 1\n",
        "df_trades_one.loc[( (df_trades_one[\"MACD\"] - df_trades_one[\"signal\"]) > 0 ), \"trade_signal\"] = -1\n",
        "\n",
        "# Calculate our portfolio returns as well as buy and hold\n",
        "df_trades_one[\"Strategy Daily Returns\"] = df_trades_one[\"ATVI\"] * df_trades_one[\"trade_signal\"].shift()\n",
        "\n",
        "df_trades_one[\"ATVI Strategy Returns\"] = (1 + df_trades_one[\"Strategy Daily Returns\"]).cumprod()\n",
        "\n",
        "df_trades_one = df_trades_one.dropna()\n",
        "\n",
        "df_trades_one[\"ATVI Buy and Hold\"] = (1 + df_trades_one[\"ATVI\"]).cumprod()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_trades_one\n",
        "# df_trades_one[\"trade_signal\"].value_counts() this strategy was short about half the time and long about half the time 128 days long and 121 days short"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeKPsiVl_rAA"
      },
      "source": [
        "# Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJz4Yz7OQY4M"
      },
      "source": [
        "# This is function takes in our price data and calculates the Compound Annual Growth Rate, Maximum Percent Draw Down, \n",
        "# the MAR Ratio (a meaure of annual growth over worst pullback) and the sharpe ratio for any one ticker. It then outputs the data\n",
        "# in a pandas dataframe with the metrics as the index and the ticker as the column.\n",
        "\n",
        "# to use the function you have to pass in a dataframe of just the closing prices with the column labled with the tickers\n",
        "# and a list of tickers corresponding to those column names\n",
        "\n",
        "def calculate_perf_stats(ticker_prices_df, ticker_list):\n",
        " \n",
        "  # Establish the Dataframe that will contain all of the output information\n",
        "\n",
        "  row_list = [\"CAGR\", \"Max Drawdown\", \"Annual Volatility\", \"MAR Ratio\", \"Sharpe Ratio\"]\n",
        "  output_df = pd.DataFrame(index=row_list, columns=ticker_list)\n",
        "\n",
        "  \n",
        "  # Create a for loop to go through each ticker in the ticker list\n",
        "  for ticker in ticker_list:\n",
        "\n",
        "    # establish the dataframe that will be used in this function\n",
        "    \n",
        "    df_prices = pd.DataFrame()\n",
        "    df_prices[ticker] = ticker_prices_df[ticker].copy()\n",
        "    df_prices.index = ticker_prices_df.index\n",
        "\n",
        "    df_prices[f\"{ticker}_daily_return\"] = df_prices[ticker].pct_change().dropna()\n",
        "\n",
        "    # Calculate Compound Annual Growth Rate (CAGR) for ticker\n",
        "\n",
        "    days_traded = df_prices[ticker].count() + 1\n",
        "    days_per_year = 252\n",
        "    \n",
        "    end_value = df_prices[ticker][-1]\n",
        "    start_value = df_prices[ticker][0]\n",
        "\n",
        "    CAGR = ((end_value / start_value)**(1/(days_traded/days_per_year))) - 1\n",
        "\n",
        "    # Calculate Maximum Drawdown\n",
        "\n",
        "    top_price = 0.0 \n",
        "\n",
        "    top_dollar = []\n",
        "\n",
        "    for value in df_prices[ticker]:\n",
        "      if value > top_price:\n",
        "        top_price = value\n",
        "      \n",
        "      top_dollar.append(top_price)\n",
        "\n",
        "    df_prices[\"top_dollar\"] = top_dollar\n",
        "\n",
        "    df_prices[\"draw_down\"] = abs((df_prices[ticker] - df_prices[\"top_dollar\"]) / df_prices[\"top_dollar\"])\n",
        "\n",
        "    max_dd = df_prices[\"draw_down\"].max()\n",
        "\n",
        "    # Calculates the MAR Ratio which is the annual growth rate over largest drawdown exposure\n",
        "\n",
        "    mar_ratio = (CAGR / max_dd)\n",
        "\n",
        "    \n",
        "    # Calculates the Annualized Return and standard deviation and combines them to calculate the sharpe ratio\n",
        "    \n",
        "    annual_ret = df_prices[f\"{ticker}_daily_return\"].mean() * days_per_year\n",
        "    annual_vol = df_prices[f\"{ticker}_daily_return\"].std() * np.sqrt(days_per_year)\n",
        "\n",
        "    sharpe = annual_ret / annual_vol\n",
        "    \n",
        "    # Add calculations to the output dataframe\n",
        "\n",
        "    output_df[ticker][\"CAGR\"] = CAGR\n",
        "    output_df[ticker][\"Max Drawdown\"] = max_dd\n",
        "    output_df[ticker][\"Annual Volatility\"] = annual_vol\n",
        "    output_df[ticker][\"MAR Ratio\"] = mar_ratio\n",
        "    output_df[ticker][\"Sharpe Ratio\"] = sharpe\n",
        "\n",
        "  return output_df\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JQ3fYsK8Hwo"
      },
      "source": [
        "\n",
        "# Here is the performance metrics dataframe for the buy and hold positions\n",
        "\n",
        "\n",
        "buy_and_hold_df = calculate_perf_stats(closing_prices_df, [\"SPY\", \"ATVI\"])\n",
        "\n",
        "print(\"Buy and Hold performance Metrics\")\n",
        "buy_and_hold_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slwxtHmcVr0E"
      },
      "source": [
        "# Results from our first Strategy test\n",
        "\n",
        "first_trade_test_df = calculate_perf_stats(df_trades_one, [\"ATVI Buy and Hold\", \"ATVI Strategy Returns\"])\n",
        "\n",
        "first_trade_test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_KxwwxzWoYH"
      },
      "source": [
        "# Performance Chart\n",
        "df_trades_one[[\"ATVI Buy and Hold\", \"ATVI Strategy Returns\"]].plot(figsize=(15,7), title=\"Cumulative Returns\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}